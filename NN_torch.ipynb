{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxleygJkQvyAsZAyYOpc1T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-gFaVBZLXBG",
        "colab_type": "code",
        "outputId": "b8a41eaa-e3e3-4c27-d34e-9bb360ba27c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "#Shapes\n",
        "#x_train: (60000, 28, 28)\n",
        "#x_test: (10000, 28, 28)\n",
        "#y_train: (60000,)\n",
        "#y_test: (10000,)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "trainset_labels = y_train\n",
        "testset_labels = y_test\n",
        "\n",
        "x_train, x_test = x_train.T.reshape((784,60000)), x_test.T.reshape((784,10000))\n",
        "x_train = x_train/255 \n",
        "x_test = x_test/255 \n",
        "\n",
        "train_labels = []\n",
        "for i in range(len(y_train)):\n",
        "    arr = [0,]*10\n",
        "    label = y_train[i]\n",
        "    index = label-1\n",
        "    arr[index]=1\n",
        "    train_labels.append(arr)\n",
        "    \n",
        "test_labels = []\n",
        "for i in range(len(y_test)):\n",
        "    arr = [0,]*10\n",
        "    label = y_train[i]\n",
        "    index = label-1\n",
        "    arr[index]=1\n",
        "    test_labels.append(arr)\n",
        "    \n",
        "y_train = np.array(train_labels).T\n",
        "y_test = np.array(test_labels).T\n",
        "\n",
        "print(\"X_train:\",x_train.shape, \"Y_train:\",y_train.shape, \"X_test:\",x_test.shape, \"Y_test:\",y_test.shape)\n",
        "    \n",
        "#Shapes\n",
        "#x_train: (784, 60000), 784 = 28px * 28px\n",
        "#x_test: (784, 10000), 784 = 28px * 28px\n",
        "#y_train: (10, 60000), 10 = No. of classes \n",
        "#y_test: (10, 10000), 10 = No. of classes\n",
        "\n",
        "x_train = torch.from_numpy(x_train).float().cuda()\n",
        "y_train = torch.from_numpy(y_train).float().cuda()\n",
        "x_test = torch.from_numpy(x_test).float().cuda()\n",
        "y_test = torch.from_numpy(y_test).float().cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (784, 60000) Y_train: (10, 60000) X_test: (784, 10000) Y_test: (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Jf26i1L957",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \n",
        "    def __init__(self, n_units, activation=\"relu\", cuda=True):\n",
        "        self.n_units = n_units\n",
        "        \n",
        "        assert(activation in (\"relu\",\"sigmoid\", \"tanh\", \"leaky-relu\")), \"InitError: Invalid Activation Function\"\n",
        "        \n",
        "        self.activationFunction = activation\n",
        "        self.weights = None\n",
        "        self.biases = None\n",
        "        self.activations = None\n",
        "        self.Z = None\n",
        "        self.cuda = cuda\n",
        "        \n",
        "        \n",
        "    def applyActivationFunction(self, Z):\n",
        "        if self.activationFunction==\"sigmoid\":\n",
        "            return 1 / (1 + torch.exp(-Z))\n",
        "\n",
        "        elif self.activationFunction==\"relu\":\n",
        "            if self.cuda:\n",
        "                return torch.max(Z, torch.zeros(Z.shape).cuda())\n",
        "            else:\n",
        "                return torch.max(Z, torch.zeros(Z.shape))\n",
        "\n",
        "        elif self.activationFunction==\"leaky-relu\":\n",
        "            if self.cuda:\n",
        "                return torch.max(0.01*Z, torch.zeros(Z.shape).cuda())\n",
        "            else:\n",
        "                return torch.max(0.01*Z, torch.zeros(Z.shape))\n",
        "\n",
        "        elif self.activationFunction==\"tanh\":\n",
        "            return torch.tanh(Z)\n",
        "    \n",
        "    \n",
        "    def isInvalid(self, A):\n",
        "        if torch.isnan(A).any() or torch.isinf(A).any():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    \n",
        "    def computeActivations(self, A):\n",
        "        assert(self.weights.shape[1]==A.shape[0]), \"ComputeActivationsError: Incorrect Input Dimensions\"\n",
        "        \n",
        "        Z = torch.matmul(self.weights,A)+self.biases\n",
        "        self.Z = Z\n",
        "        self.activations = self.applyActivationFunction(Z)\n",
        "        \n",
        "        assert(not self.isInvalid(self.activations)), \"InvalidActivationsError: Invalid Activations Computed\"\n",
        "        return self.activations  \n",
        "\n",
        "    \n",
        "    def computeDerivatives(self):\n",
        "        if self.activationFunction==\"sigmoid\":\n",
        "            sigmoidZ = self.activations\n",
        "            derivative = torch.mul(sigmoidZ, (1-sigmoidZ))\n",
        "\n",
        "        elif self.activationFunction==\"relu\":\n",
        "            ones = torch.ones(self.Z.shape)\n",
        "            zeros = torch.zeros(self.Z.shape)\n",
        "            if self.cuda:\n",
        "                ones = ones.cuda()\n",
        "                zeros = zeros.cuda()\n",
        "            derivative = torch.where(self.Z<0, zeros,ones)\n",
        "\n",
        "        elif self.activationFunction==\"leaky-relu\":\n",
        "            ones = torch.ones(self.Z.shape)\n",
        "            zeros = torch.ones(self.Z.shape)*0.01\n",
        "            if self.cuda:\n",
        "                ones = ones.cuda()\n",
        "                zeros = zeros.cuda()\n",
        "            derivative = torch.where(self.Z<0, zeros,ones)\n",
        "\n",
        "        elif self.activationFunction==\"tanh\":\n",
        "            tanhZ = self.activation(self.Z)\n",
        "            derivative = 1 - torch.pow(tanhZ, 2)\n",
        "\n",
        "        return derivative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltTqQLEsZgO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN():\n",
        "\n",
        "    def __init__(self, learningRate=0.005, lossFunction = \"cross-entropy\", cuda=True):\n",
        "        self.layerDims = []\n",
        "        self.layers = []\n",
        "        self.nLayers = 0\n",
        "        self.learningRate = learningRate\n",
        "        self.lossFunction = lossFunction\n",
        "        self.cuda = cuda\n",
        "                \n",
        "\n",
        "    def addLayer(self, layer):\n",
        "        layer.cuda = self.cuda\n",
        "        self.layerDims.append(layer.n_units)\n",
        "        self.layers.append(layer)\n",
        "        self.nLayers+=1\n",
        "\n",
        "\n",
        "    def initializeParams(self, weights=None):\n",
        "        assert(len(self.layerDims)>1), \"NotEnoughLayersException: Network must have atleast 2 layers\"\n",
        "        \n",
        "        if weights==None:\n",
        "            for i in range(1, self.nLayers):\n",
        "                layer = self.layers[i]\n",
        "                if self.cuda:\n",
        "                    layer.weights = (torch.rand(self.layerDims[i], self.layerDims[i-1]) * 0.01).cuda()\n",
        "                    layer.biases = torch.zeros(self.layerDims[i], 1).cuda()\n",
        "                else:\n",
        "                    layer.weights = torch.rand(self.layerDims[i], self.layerDims[i-1]) * 0.01\n",
        "                    layer.biases = torch.zeros(self.layerDims[i], 1)\n",
        "        else:\n",
        "            for i in range(1, self.nLayers):\n",
        "                layer = self.layers[i]\n",
        "                layer.weights = weights[i]['weights']\n",
        "                layer.biases = weights[i]['biases']\n",
        "\n",
        "    \n",
        "    def forward(self, trainingExamples):\n",
        "        activation = trainingExamples\n",
        "        for i in range(1, self.nLayers):\n",
        "            layer = self.layers[i]\n",
        "            activation = layer.computeActivations(activation)\n",
        "            layer.activations = activation\n",
        "\n",
        "\n",
        "    def getAvgLoss(self, Y):\n",
        "        m = Y.shape[1]\n",
        "        outputLayer = self.layers[-1]\n",
        "        A = outputLayer.activations \n",
        "        \n",
        "        assert(Y.shape == A.shape), \"InvalidLabelFormatException: Labels should have dimensions (nClasses, nExamples)\"\n",
        "        \n",
        "        cost = -(1/m)*torch.sum(torch.mul(Y,torch.log(A)) + torch.mul((1-Y),torch.log(1-A)))  \n",
        "        return cost\n",
        "    \n",
        "\n",
        "    \"\"\" Initialize backward propagation \"\"\"\n",
        "    def computeDA(self, Y):\n",
        "        outputLayer = self.layers[-1]\n",
        "        A = outputLayer.activations\n",
        "        if self.lossFunction==\"cross-entropy\":\n",
        "            DA =  - (torch.div(Y, A) - torch.div(1 - Y, 1 - A))\n",
        "        return DA\n",
        "\n",
        "\n",
        "    def backward(self, labels):\n",
        "        \"\"\" Output Layer's DA \"\"\"\n",
        "        \n",
        "        DA = self.computeDA(labels)\n",
        "        for i in range(self.nLayers-1,0,-1):\n",
        "            layer = self.layers[i]\n",
        "            DZ = torch.mul(DA,layer.computeDerivatives())\n",
        "            DA = torch.matmul(layer.weights.t(), DZ)\n",
        "            \n",
        "            prevLayer = self.layers[i-1]\n",
        "            m = prevLayer.activations.shape[1]\n",
        "\n",
        "            DW = (1/m)*torch.matmul(DZ, prevLayer.activations.t())\n",
        "            \"\"\" keepdims maintains the dimension of the resultant matrix. \"\"\"\n",
        "            DB = 1 / m * (torch.sum(DZ, dim = 1,keepdim = True)) \n",
        "            \n",
        "            layer.weights -=  self.learningRate*DW\n",
        "            layer.biases -= self.learningRate*DB\n",
        "            \n",
        "        \n",
        "    def train(self, dataset, labels, weights=None, stopWhenAvgLossEquals=0.05, printLossAfterEvery=50, nIterations=1000, mode='auto'):\n",
        "        \n",
        "        if weights==None:\n",
        "            self.initializeParams()\n",
        "        else:\n",
        "            self.initializeParams(weights)\n",
        "        \n",
        "        \"\"\" Normalizing the dataset. \"\"\"\n",
        "        if (dataset>1).any():\n",
        "            dataset = dataset/torch.max(dataset)\n",
        "        \n",
        "        self.layers[0].activations = dataset\n",
        "        \n",
        "        assert(mode in ('auto','manual')), \"InvalidModeException: Mode can be either 'auto' or 'manual'\"\n",
        "        \n",
        "        if mode=='manual':\n",
        "\n",
        "            for i in range(nIterations):\n",
        "                self.forward(dataset)\n",
        "\n",
        "                avgLoss = self.getAvgLoss(labels)\n",
        "\n",
        "                if i%printLossAfterEvery==0:\n",
        "                    print(\"Avg Loss at iteration \"+str(i)+\": \"+str(avgLoss))\n",
        "\n",
        "                self.backward(labels)\n",
        "        \n",
        "        elif mode==\"auto\":\n",
        "            \n",
        "            iterNo = 0\n",
        "\n",
        "            while True:\n",
        "                self.forward(dataset)\n",
        "                avgLoss = self.getAvgLoss(labels)\n",
        "\n",
        "                if iterNo%printLossAfterEvery==0:\n",
        "                    print(\"Avg Loss at iteration \"+str(iterNo)+\": \"+str(avgLoss))\n",
        "\n",
        "                nDecimalsToRound = len(str(stopWhenAvgLossEquals).split('.')[1])\n",
        "                self.backward(labels)\n",
        "\n",
        "                if stopWhenAvgLossEquals==(torch.round(avgLoss* 10**nDecimalsToRound)/(10**nDecimalsToRound)):\n",
        "                    break \n",
        "\n",
        "                iterNo+=1\n",
        "\n",
        "        print(\"\\nTraining completed successfully\")\n",
        "        \n",
        "        ''' First layer is the input layer. Hence it doesn't contain any weights. '''\n",
        "        weights = [None,]\n",
        "        \n",
        "        for i in range(1, self.nLayers):\n",
        "            layer = self.layers[i]\n",
        "            weights.append({'weights':layer.weights, 'biases':layer.biases})\n",
        "\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def predict(self, example):\n",
        "        try:\n",
        "            nExamples = example.shape[1]\n",
        "        except:\n",
        "            example = example.reshape(example.shape[0],1)\n",
        "        self.forward(example)\n",
        "        outputLayer = self.layers[-1]\n",
        "        activations = outputLayer.activations\n",
        "        return torch.argmax(activations, dim=0)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ITkdI3MD-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = NN(learningRate=0.1, cuda=True)\n",
        "n.addLayer(Layer(784))\n",
        "n.addLayer(Layer(128))\n",
        "n.addLayer(Layer(64))\n",
        "n.addLayer(Layer(10,activation=\"sigmoid\"))\n",
        "\n",
        "n.learningRate=0.05"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj9_oyLhMFfO",
        "colab_type": "code",
        "outputId": "1e633bb0-1761-498e-87dd-d7c582b068a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "weights =  n.train(x_train, y_train, nIterations=500, printLossAfterEvery=100, mode='manual', weights=None)\n",
        "weights =  n.train(x_train, y_train, nIterations=500, printLossAfterEvery=100, mode='manual', weights=weights)\n",
        "\n",
        "total = y_test.shape[1]\n",
        "count = 0\n",
        "\n",
        "for i in range(total):\n",
        "    example = x_test.T[i]\n",
        "    \n",
        "    label = testset_labels[i]\n",
        "    prediction = n.predict(example)+1\n",
        "    \n",
        "    if prediction==label:\n",
        "        count+=1\n",
        "\n",
        "print(\"\\nTest Set Accuracy: \"+str((count/total)*100)+\"%\")\n",
        "\n",
        "#n.saveWeights(\"/content/sample_data/model-torch\", weights=weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg Loss at iteration 0: tensor(7.3658, device='cuda:0')\n",
            "Avg Loss at iteration 100: tensor(3.3519, device='cuda:0')\n",
            "Avg Loss at iteration 200: tensor(3.3294, device='cuda:0')\n",
            "Avg Loss at iteration 300: tensor(3.3147, device='cuda:0')\n",
            "Avg Loss at iteration 400: tensor(3.3025, device='cuda:0')\n",
            "\n",
            "Training completed successfully\n",
            "Avg Loss at iteration 0: tensor(3.2916, device='cuda:0')\n",
            "Avg Loss at iteration 100: tensor(3.2815, device='cuda:0')\n",
            "Avg Loss at iteration 200: tensor(3.2719, device='cuda:0')\n",
            "Avg Loss at iteration 300: tensor(3.2625, device='cuda:0')\n",
            "Avg Loss at iteration 400: tensor(3.2526, device='cuda:0')\n",
            "\n",
            "Training completed successfully\n",
            "\n",
            "Test Set Accuracy: 16.259999999999998%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}